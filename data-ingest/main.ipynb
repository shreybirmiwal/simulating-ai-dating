{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_date_me_directory():\n",
    "    # Fetch the webpage\n",
    "    url = 'https://dateme.directory/browse'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'docs'})\n",
    "    \n",
    "    if not table:\n",
    "        print(\"Could not find the data table\")\n",
    "        return None\n",
    "\n",
    "    # Prepare data storage\n",
    "    data = []\n",
    "    columns = ['Name', 'ProfileLink', 'Age', 'Gender', 'InterestedIn', 'Style', \n",
    "               'Location', 'LocationFlexibility', 'Contact', 'LastUpdated']\n",
    "\n",
    "    # Extract table rows\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) != 9:\n",
    "            continue\n",
    "\n",
    "        # Extract name and link\n",
    "        name_link = cells[0].find('a')\n",
    "        name = name_link.text.strip() if name_link else cells[0].div.text.strip()\n",
    "        profile_link = name_link['href'] if name_link else ''\n",
    "\n",
    "        # Extract remaining data\n",
    "        row_data = {\n",
    "            'Name': name,\n",
    "            'ProfileLink': profile_link,\n",
    "            'Age': cells[1].div.text.strip(),\n",
    "            'Gender': cells[2].div.text.strip(),\n",
    "            'InterestedIn': ', '.join([span.text.strip() for span in cells[3].div.find_all('span')]) or cells[3].div.text.strip(),\n",
    "            'Style': ', '.join([span.text.strip() for span in cells[4].div.find_all('span')]) or cells[4].div.text.strip(),\n",
    "            'Location': ', '.join([span.text.strip() for span in cells[5].div.find_all('span')]) or cells[5].div.text.strip(),\n",
    "            'LocationFlexibility': cells[6].div.text.strip(),\n",
    "            'Contact': cells[7].div.text.strip(),\n",
    "            'LastUpdated': cells[8].div.text.strip()\n",
    "        }\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing profile links...\n",
      "\n",
      "Sample results:\n",
      "         Name                                        ProfileLink  \\\n",
      "0       Gabin  https://docs.google.com/document/d/1ymul6X3DQH...   \n",
      "1   Malvin G.  https://docs.google.com/document/d/1zuWjBGbGVz...   \n",
      "2      Callum  https://docs.google.com/document/d/16Li0PY9WG5...   \n",
      "3  Ari Zerner                     https://arizerner.com/date-me/   \n",
      "4      Aeneas  https://docs.google.com/document/d/e/2PACX-1vS...   \n",
      "\n",
      "                                         ProfileText  \n",
      "0  ﻿Hi!\\r\\nThis is a dating doc, I’m a 25 year ol...  \n",
      "1                 Could not extract document content  \n",
      "2  ﻿In as many words\\r\\n\\r\\n\\r\\nTo love and be lo...  \n",
      "3  Ari's Date Me Doc | Ari Zerner's Demesne\\nAri ...  \n",
      "4                 Could not extract document content  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_google_doc_text(url):\n",
    "    \"\"\"Extract text from a Google Docs URL\"\"\"\n",
    "    try:\n",
    "        # Convert to export URL\n",
    "        doc_id = url.split('/d/')[1].split('/')[0]\n",
    "        export_url = f'https://docs.google.com/document/d/{doc_id}/export?format=txt'\n",
    "        \n",
    "        response = requests.get(export_url, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        return \"Could not extract document content\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error extracting document: {str(e)}\"\n",
    "\n",
    "def extract_general_page_text(url):\n",
    "    \"\"\"Extract text from a general webpage\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup.get_text(separator='\\n', strip=True)\n",
    "        return \"Could not extract page content\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error extracting page: {str(e)}\"\n",
    "\n",
    "def get_dating_data(df_row):\n",
    "    link = df_row['ProfileLink']\n",
    "    if not link:\n",
    "        df_row['ProfileText'] = 'No link available'\n",
    "        return df_row\n",
    "    \n",
    "    try:\n",
    "        # Check if Google Doc\n",
    "        if 'docs.google.com/document/' in link:\n",
    "            text = extract_google_doc_text(link)\n",
    "        else:\n",
    "            text = extract_general_page_text(link)\n",
    "        \n",
    "        df_row['ProfileText'] = text\n",
    "        time.sleep(1)  # Be polite with requests\n",
    "        \n",
    "    except Exception as e:\n",
    "        df_row['ProfileText'] = f\"Error processing link: {str(e)}\"\n",
    "    \n",
    "    return df_row\n",
    "\n",
    "# Modify your main block to use this:\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_date_me_directory()\n",
    "    if df is not None:\n",
    "        print(\"Processing profile links...\")\n",
    "        # Process first 5 rows as example (remove [0:5] to process all)\n",
    "        df = df.apply(get_dating_data, axis=1)\n",
    "        \n",
    "        print(\"\\nSample results:\")\n",
    "        print(df[['Name', 'ProfileLink', 'ProfileText']].head())\n",
    "        \n",
    "        # To save full results:\n",
    "        # df.to_csv('date_me_directory_with_text.csv', index=False)\n",
    "    else:\n",
    "        print(\"Failed to scrape data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('date_me_directory_with_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>ProfileLink</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>InterestedIn</th>\n",
       "      <th>Style</th>\n",
       "      <th>Location</th>\n",
       "      <th>LocationFlexibility</th>\n",
       "      <th>Contact</th>\n",
       "      <th>LastUpdated</th>\n",
       "      <th>ProfileText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gabin</td>\n",
       "      <td>https://docs.google.com/document/d/1ymul6X3DQH...</td>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "      <td>F, NB</td>\n",
       "      <td>Any</td>\n",
       "      <td>Central Europe, South of France</td>\n",
       "      <td>Some</td>\n",
       "      <td>GabinDatingDoc@proton.me</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>﻿Hi!\\r\\nThis is a dating doc, I’m a 25 year ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malvin G.</td>\n",
       "      <td>https://docs.google.com/document/d/1zuWjBGbGVz...</td>\n",
       "      <td>34</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>mono</td>\n",
       "      <td>San Francisco Bay Area, NYC, Central Europe, D...</td>\n",
       "      <td>Flexible</td>\n",
       "      <td>https://docs.google.com/forms/d/1-opeZgl3qdr4p...</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>Could not extract document content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Callum</td>\n",
       "      <td>https://docs.google.com/document/d/16Li0PY9WG5...</td>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>mono</td>\n",
       "      <td>UK</td>\n",
       "      <td>Flexible</td>\n",
       "      <td>callumcallum109@outlook.com</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>﻿In as many words\\r\\n\\r\\n\\r\\nTo love and be lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ari Zerner</td>\n",
       "      <td>https://arizerner.com/date-me/</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>F, NB</td>\n",
       "      <td>poly</td>\n",
       "      <td>San Francisco Bay Area, Philadelphia</td>\n",
       "      <td>Some</td>\n",
       "      <td>ari@zerner.com</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Ari's Date Me Doc | Ari Zerner's Demesne\\nAri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aeneas</td>\n",
       "      <td>https://docs.google.com/document/d/e/2PACX-1vS...</td>\n",
       "      <td>29</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>mono</td>\n",
       "      <td>Flexible</td>\n",
       "      <td>Flexible</td>\n",
       "      <td>aokook@gmail.com</td>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>Could not extract document content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Blaïse (bless)</td>\n",
       "      <td>https://docs.google.com/document/d/1ndGs9G9SvG...</td>\n",
       "      <td>38</td>\n",
       "      <td>NB</td>\n",
       "      <td>M</td>\n",
       "      <td>mono</td>\n",
       "      <td>Ladysmith BC</td>\n",
       "      <td>None</td>\n",
       "      <td>Academia_nut@shaw.ca</td>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>﻿Dating Stephen Blaïse Saint Clare\\r\\n\\r\\n\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Brian Tomasik</td>\n",
       "      <td>https://briantomasik.com/my-dating-profile/</td>\n",
       "      <td>35</td>\n",
       "      <td>M</td>\n",
       "      <td>F, NB</td>\n",
       "      <td>mono</td>\n",
       "      <td>NY</td>\n",
       "      <td>Flexible</td>\n",
       "      <td></td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>My dating profile\\nHome\\n•\\nWritings\\nMy datin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Wes</td>\n",
       "      <td>https://docs.google.com/document/d/1w2jH3CRis1...</td>\n",
       "      <td>39</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>poly</td>\n",
       "      <td>Collingswood, NJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>﻿BESPOKE 1983 NONMONOGAMOUS DAD MODEL, AVAILAB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Mercer</td>\n",
       "      <td>https://docs.google.com/document/d/1FUcf4P06yV...</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NYC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>Could not extract document content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>Bhargab Acharya</td>\n",
       "      <td>https://bhargab.com.np/posts/dating-profile/</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>Could not extract page content</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name                                        ProfileLink Age  \\\n",
       "0              Gabin  https://docs.google.com/document/d/1ymul6X3DQH...  25   \n",
       "1          Malvin G.  https://docs.google.com/document/d/1zuWjBGbGVz...  34   \n",
       "2             Callum  https://docs.google.com/document/d/16Li0PY9WG5...  40   \n",
       "3         Ari Zerner                     https://arizerner.com/date-me/  27   \n",
       "4             Aeneas  https://docs.google.com/document/d/e/2PACX-1vS...  29   \n",
       "..               ...                                                ...  ..   \n",
       "477   Blaïse (bless)  https://docs.google.com/document/d/1ndGs9G9SvG...  38   \n",
       "478    Brian Tomasik        https://briantomasik.com/my-dating-profile/  35   \n",
       "479              Wes  https://docs.google.com/document/d/1w2jH3CRis1...  39   \n",
       "480           Mercer  https://docs.google.com/document/d/1FUcf4P06yV...  22   \n",
       "481  Bhargab Acharya       https://bhargab.com.np/posts/dating-profile/  27   \n",
       "\n",
       "    Gender InterestedIn Style  \\\n",
       "0        M        F, NB   Any   \n",
       "1        M            F  mono   \n",
       "2        M            F  mono   \n",
       "3        M        F, NB  poly   \n",
       "4        M            F  mono   \n",
       "..     ...          ...   ...   \n",
       "477     NB            M  mono   \n",
       "478      M        F, NB  mono   \n",
       "479      M            F  poly   \n",
       "480      M                      \n",
       "481      M                      \n",
       "\n",
       "                                              Location LocationFlexibility  \\\n",
       "0                      Central Europe, South of France                Some   \n",
       "1    San Francisco Bay Area, NYC, Central Europe, D...            Flexible   \n",
       "2                                                   UK            Flexible   \n",
       "3                 San Francisco Bay Area, Philadelphia                Some   \n",
       "4                                             Flexible            Flexible   \n",
       "..                                                 ...                 ...   \n",
       "477                                       Ladysmith BC                None   \n",
       "478                                                 NY            Flexible   \n",
       "479                                   Collingswood, NJ                       \n",
       "480                                                NYC                       \n",
       "481                                                                          \n",
       "\n",
       "                                               Contact LastUpdated  \\\n",
       "0                             GabinDatingDoc@proton.me  2025-03-07   \n",
       "1    https://docs.google.com/forms/d/1-opeZgl3qdr4p...  2025-03-07   \n",
       "2                          callumcallum109@outlook.com  2025-03-04   \n",
       "3                                       ari@zerner.com  2025-02-24   \n",
       "4                                     aokook@gmail.com  2025-02-20   \n",
       "..                                                 ...         ...   \n",
       "477                               Academia_nut@shaw.ca  2022-10-14   \n",
       "478                                                     2022-10-01   \n",
       "479                                                     2022-09-29   \n",
       "480                                                     2022-09-29   \n",
       "481                                                     2022-09-29   \n",
       "\n",
       "                                           ProfileText  \n",
       "0    ﻿Hi!\\r\\nThis is a dating doc, I’m a 25 year ol...  \n",
       "1                   Could not extract document content  \n",
       "2    ﻿In as many words\\r\\n\\r\\n\\r\\nTo love and be lo...  \n",
       "3    Ari's Date Me Doc | Ari Zerner's Demesne\\nAri ...  \n",
       "4                   Could not extract document content  \n",
       "..                                                 ...  \n",
       "477  ﻿Dating Stephen Blaïse Saint Clare\\r\\n\\r\\n\\r\\n...  \n",
       "478  My dating profile\\nHome\\n•\\nWritings\\nMy datin...  \n",
       "479  ﻿BESPOKE 1983 NONMONOGAMOUS DAD MODEL, AVAILAB...  \n",
       "480                 Could not extract document content  \n",
       "481                     Could not extract page content  \n",
       "\n",
       "[482 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
